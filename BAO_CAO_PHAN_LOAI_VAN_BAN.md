# BÃO CÃO: PHÃ‚N LOáº I VÄ‚N Báº¢N TIN Tá»¨C TÃ€I CHÃNH TIáº¾NG VIá»†T (ARGUMENT CLASSIFICATION)

---

## 1. Tá»”NG QUAN Dá»° ÃN

### 1.1 Má»¥c tiÃªu

XÃ¢y dá»±ng vÃ  so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Deep Learning trong bÃ i toÃ¡n **phÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t** - cá»¥ thá»ƒ lÃ  phÃ¢n loáº¡i cÃ¡c Ä‘oáº¡n tin tá»©c tÃ i chÃ­nh thÃ nh cÃ¡c loáº¡i láº­p luáº­n (Argument Classification).

### 1.2 Bá»™ dá»¯ liá»‡u

- **Nguá»“n dá»¯ liá»‡u**: File Excel chá»©a tin tá»©c tÃ i chÃ­nh tiáº¿ng Viá»‡t
- **Tá»•ng máº«u ban Ä‘áº§u**: 16,975 máº«u
- **Sau khi lÃ m sáº¡ch**: 16,959 máº«u (loáº¡i bá» 16 máº«u cÃ³ giÃ¡ trá»‹ thiáº¿u)
- **CÃ¡c cá»™t dá»¯ liá»‡u chÃ­nh**: `text` (ná»™i dung vÄƒn báº£n), `argument_classification_label` (nhÃ£n phÃ¢n loáº¡i)

### 1.3 PhÃ¢n bá»‘ nhÃ£n (3 lá»›p)

| NhÃ£n         | Sá»‘ lÆ°á»£ng | Tá»· lá»‡  |
| ------------ | -------- | ------ |
| **Tiá»n Ä‘á»**  | 11,547   | 68.09% |
| **Ká»‹ch báº£n** | 3,150    | 18.57% |
| **Káº¿t luáº­n** | 2,262    | 13.34% |

> **Nháº­n xÃ©t**: Dá»¯ liá»‡u cÃ³ sá»± máº¥t cÃ¢n báº±ng Ä‘Ã¡ng ká»ƒ, vá»›i lá»›p "Tiá»n Ä‘á»" chiáº¿m Ä‘a sá»‘ (~68%).

### 1.4 Thá»‘ng kÃª vÄƒn báº£n

- Äá»™ dÃ i trung bÃ¬nh: **386.7 kÃ½ tá»±**
- Sá»‘ tá»« trung bÃ¬nh: **84.5 tá»«**
- Sá»‘ tá»« tá»‘i Ä‘a: **1,083 tá»«**
- PhÃ¢n vá»‹ 95%: **178 tá»«**

---

## 2. PHÆ¯Æ NG PHÃP THá»°C HIá»†N

### 2.1 Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

1. Loáº¡i bá» giÃ¡ trá»‹ thiáº¿u
2. LÃ m sáº¡ch vÄƒn báº£n (lowercase, loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t)
3. MÃ£ hÃ³a nhÃ£n báº±ng `LabelEncoder`
4. XÃ¢y dá»±ng tá»« Ä‘iá»ƒn tá»« táº­p huáº¥n luyá»‡n (tá»« xuáº¥t hiá»‡n â‰¥ 2 láº§n)

### 2.2 Tham sá»‘ cáº¥u hÃ¬nh

| Tham sá»‘                | GiÃ¡ trá»‹ |
| ---------------------- | ------- |
| KÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn     | 5,543   |
| Äá»™ dÃ i chuá»—i tá»‘i Ä‘a    | 100     |
| Embedding dimension    | 128     |
| Sá»‘ heads (Transformer) | 8       |
| Feed-forward dimension | 256     |

### 2.3 Chia táº­p dá»¯ liá»‡u

| Táº­p        | Sá»‘ lÆ°á»£ng | Tá»· lá»‡ |
| ---------- | -------- | ----- |
| Training   | 11,871   | 70%   |
| Validation | 2,544    | 15%   |
| Test       | 2,544    | 15%   |

---

## 3. CÃC MÃ” HÃŒNH ÄÃƒ XÃ‚Y Dá»°NG

### 3.1 LSTM (Long Short-Term Memory)

- 2 lá»›p LSTM vá»›i 256 units má»—i lá»›p
- Dropout: 0.3
- Optimizer: Adam (lr=0.001)

### 3.2 BiLSTM (Bidirectional LSTM)

- 2 lá»›p Bidirectional LSTM vá»›i 256 units
- Dropout: 0.3
- Optimizer: Adam (lr=0.001)

### 3.3 GRU (Gated Recurrent Unit)

- 2 lá»›p GRU vá»›i 256 units má»—i lá»›p
- Dropout: 0.3
- Optimizer: Adam (lr=0.001)

### 3.4 Transformer

- 2 Transformer blocks
- Multi-Head Attention (8 heads)
- Global Average Pooling
- Dropout: 0.3

### 3.5 PhoBERT

- Pretrained model: `vinai/phobert-base`
- Fine-tuning vá»›i classification head
- Optimizer: Adam (lr=2e-5)
- Batch size: 16 (nhá» hÆ¡n do giá»›i háº¡n bá»™ nhá»›)

---

## 4. QUÃ TRÃŒNH HUáº¤N LUYá»†N

### 4.1 Cáº¥u hÃ¬nh huáº¥n luyá»‡n

- **Epochs**: 30
- **Batch size**: 32 (16 cho PhoBERT)
- **Callbacks**: ReduceLROnPlateau (factor=0.5, patience=2, min_lr=1e-6)

### 4.2 Thá»i gian huáº¥n luyá»‡n (GPU T4)

- LSTM: ~7s/epoch
- BiLSTM: ~13s/epoch
- GRU: ~6s/epoch
- Transformer: ~8s/epoch
- PhoBERT: ~105s/epoch

---

## 5. Káº¾T QUáº¢ ÄÃNH GIÃ TRÃŠN Táº¬P TEST

### 5.1 Báº£ng so sÃ¡nh hiá»‡u suáº¥t

| MÃ´ hÃ¬nh     | Accuracy   | F1-Score   | Sá»‘ tham sá»‘ |
| ----------- | ---------- | ---------- | ---------- |
| LSTM        | 0.7614     | 0.7548     | 1,629,827  |
| BiLSTM      | 0.7535     | 0.7464     | 3,074,435  |
| **GRU** ğŸ†  | **0.7704** | **0.7664** | 1,401,475  |
| Transformer | 0.7653     | 0.7595     | 1,897,731  |
| PhoBERT     | 0.6808     | 0.5515     | 2,307\*    |

> \*PhoBERT chá»‰ Ä‘áº¿m tham sá»‘ cá»§a classification head (pretrained weights bá»‹ Ä‘Ã³ng bÄƒng)

### 5.2 PhÃ¢n tÃ­ch káº¿t quáº£

#### ğŸ† MÃ´ hÃ¬nh tá»‘t nháº¥t: **GRU**

- **F1-Score**: 0.7664
- **Accuracy**: 77.04%
- **Sá»‘ tham sá»‘**: Ãt nháº¥t trong cÃ¡c mÃ´ hÃ¬nh RNN (1.4M)

#### Nháº­n xÃ©t:

1. **GRU** Ä‘áº¡t hiá»‡u suáº¥t cao nháº¥t vá»›i sá»‘ tham sá»‘ Ã­t nháº¥t â†’ hiá»‡u quáº£ nháº¥t
2. **LSTM vÃ  Transformer** cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng (~75.5% F1)
3. **BiLSTM** khÃ´ng cáº£i thiá»‡n so vá»›i LSTM Ä‘Æ¡n hÆ°á»›ng, cÃ³ thá»ƒ do overfitting
4. **PhoBERT** Ä‘áº¡t káº¿t quáº£ tháº¥p nháº¥t (68% accuracy, 55% F1) - Ä‘iá»u nÃ y báº¥t thÆ°á»ng vÃ  cÃ³ thá»ƒ do:
   - Learning rate quÃ¡ cao/tháº¥p
   - Cáº§n fine-tuning cáº£ pretrained layers
   - Dá»¯ liá»‡u domain-specific khÃ´ng phÃ¹ há»£p vá»›i pretrained weights chung

---

## 6. HÃŒNH áº¢NH TRá»°C QUAN

Notebook Ä‘Ã£ táº¡o cÃ¡c biá»ƒu Ä‘á»“ sau:

1. **label_distribution.png** - PhÃ¢n bá»‘ nhÃ£n
2. **training_history.png** - Loss vÃ  Accuracy theo epoch
3. **confusion_matrices.png** - Ma tráº­n nháº§m láº«n cho tá»«ng mÃ´ hÃ¬nh
4. **model_comparison.png** - So sÃ¡nh hiá»‡u suáº¥t cÃ¡c mÃ´ hÃ¬nh

---

## 7. THá»¬ NGHIá»†M Dá»° ÄOÃN (INFERENCE DEMO)

Má»™t sá»‘ vÃ­ dá»¥ dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh GRU:

| VÄƒn báº£n                                                                 | Dá»± Ä‘oÃ¡n  |
| ----------------------------------------------------------------------- | -------- |
| "ChÃºng tÃ´i khuyáº¿n nghá»‹ MUA cá»• phiáº¿u VNM vá»›i giÃ¡ má»¥c tiÃªu 85,000 VND..." | Káº¿t luáº­n |
| "Doanh thu quÃ½ 3 tÄƒng 25% so vá»›i cÃ¹ng ká»³ nÄƒm trÆ°á»›c..."                  | Tiá»n Ä‘á»  |
| "Náº¿u lÃ£i suáº¥t tiáº¿p tá»¥c tÄƒng trong 6 thÃ¡ng tá»›i..."                       | Ká»‹ch báº£n |
| "CÃ´ng ty hoáº¡t Ä‘á»™ng trong lÄ©nh vá»±c sáº£n xuáº¥t thÃ©p"                        | Tiá»n Ä‘á»  |

---

## 8. CÃC FILE Äáº¦U RA

| File                      | MÃ´ táº£                             |
| ------------------------- | --------------------------------- |
| `lstm_model.keras`        | MÃ´ hÃ¬nh LSTM Ä‘Ã£ huáº¥n luyá»‡n        |
| `bilstm_model.keras`      | MÃ´ hÃ¬nh BiLSTM Ä‘Ã£ huáº¥n luyá»‡n      |
| `gru_model.keras`         | MÃ´ hÃ¬nh GRU Ä‘Ã£ huáº¥n luyá»‡n         |
| `transformer_model.keras` | MÃ´ hÃ¬nh Transformer Ä‘Ã£ huáº¥n luyá»‡n |
| `phobert_model.keras`     | MÃ´ hÃ¬nh PhoBERT Ä‘Ã£ huáº¥n luyá»‡n     |
| `model_comparison.csv`    | Báº£ng so sÃ¡nh káº¿t quáº£              |
| `training_history.png`    | Biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh training        |
| `confusion_matrices.png`  | Ma tráº­n nháº§m láº«n                  |
| `model_comparison.png`    | Biá»ƒu Ä‘á»“ so sÃ¡nh mÃ´ hÃ¬nh           |
| `label_distribution.png`  | Biá»ƒu Ä‘á»“ phÃ¢n bá»‘ nhÃ£n              |

---

## 9. Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š

### 9.1 Káº¿t luáº­n

1. **GRU** lÃ  mÃ´ hÃ¬nh tá»‘i Æ°u nháº¥t cho bÃ i toÃ¡n nÃ y vá»›i hiá»‡u suáº¥t cao vÃ  sá»‘ tham sá»‘ Ã­t
2. CÃ¡c mÃ´ hÃ¬nh RNN truyá»n thá»‘ng (LSTM, GRU) hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n Transformer vÃ  PhoBERT trÃªn táº­p dá»¯ liá»‡u nÃ y
3. PhoBERT cáº§n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thÃªm Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n

### 9.2 Khuyáº¿n nghá»‹ cáº£i thiá»‡n

1. **Xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u**: Sá»­ dá»¥ng class weights hoáº·c oversampling
2. **Fine-tuning PhoBERT**: Thá»­ cÃ¡c learning rate khÃ¡c nhau (1e-5, 5e-6)
3. **Data augmentation**: Paraphrase hoáº·c back-translation
4. **Ensemble**: Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t
5. **Cross-validation**: ÄÃ¡nh giÃ¡ Ä‘á»™ á»•n Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh

---

## 10. THÃ”NG TIN Ká»¸ THUáº¬T

- **Framework**: TensorFlow 2.19.0
- **GPU**: Tesla T4 (Google Colab)
- **Pretrained Model**: vinai/phobert-base
- **Environment**: Google Colab

---

_BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« notebook `Untitled2.ipynb`_
